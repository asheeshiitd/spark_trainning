{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8056b2d1",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235cf20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 18:12:26 WARN Utils: Your hostname, a-Lenovo-Legion-Y530-15ICH resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlp7s0)\n",
      "22/03/28 18:12:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/03/28 18:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/28 18:12:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/28 18:12:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"asheesh\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749e291",
   "metadata": {},
   "source": [
    "### 1)StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11439695",
   "metadata": {},
   "source": [
    "StringIndexer encodes a string column of labels to a column of label indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a132c",
   "metadata": {},
   "source": [
    " id | category | categoryIndex\n",
    "----|----------|---------------\n",
    " 0  | a        | 0.0\n",
    " 1  | b        | 2.0\n",
    " 2  | c        | 1.0\n",
    " 3  | a        | 0.0\n",
    " 4  | a        | 0.0\n",
    " 5  | c        | 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ca2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0bed0",
   "metadata": {},
   "source": [
    "### 2) IndexToString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4103463",
   "metadata": {},
   "source": [
    "Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings.  A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a93132",
   "metadata": {},
   "source": [
    " id | categoryIndex | originalCategory\n",
    "----|---------------|-----------------\n",
    " 0  | 0.0           | a\n",
    " 1  | 2.0           | b\n",
    " 2  | 1.0           | c\n",
    " 3  | 0.0           | a\n",
    " 4  | 0.0           | a\n",
    " 5  | 1.0           | c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5dca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1022999",
   "metadata": {},
   "source": [
    "### 3)Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e199d0",
   "metadata": {},
   "source": [
    "Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daa1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690b5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+-----------------------------------------------------------+\n",
      "|id |features      |normFeatures                                               |\n",
      "+---+--------------+-----------------------------------------------------------+\n",
      "|0  |[1.0,0.5,-1.0]|[0.6666666666666666,0.3333333333333333,-0.6666666666666666]|\n",
      "|1  |[2.0,1.0,1.0] |[0.8164965809277261,0.4082482904638631,0.4082482904638631] |\n",
      "|2  |[4.0,10.0,2.0]|[0.3651483716701107,0.9128709291752769,0.18257418583505536]|\n",
      "+---+--------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L2 Normalization\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e32c2",
   "metadata": {},
   "source": [
    "### 4) StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baaa20",
   "metadata": {},
   "source": [
    "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "    a)withStd: True by default. Scales the data to unit standard deviation.\n",
    "    b) withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9418c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+------------------------------------------------------------+\n",
      "|rawFeatures                                                   |scaledFeatures                                              |\n",
      "+--------------------------------------------------------------+------------------------------------------------------------+\n",
      "|[0.5286404020372594,0.30841392119746824,0.9018507460856643]   |[1.7683005623340662,1.0875889439264128,3.109501342010325]   |\n",
      "|[0.5139061715002577,0.4338516311180267,0.097096322497359]     |[1.71901460529459,1.5299317082586987,0.3347795036154757]    |\n",
      "|[0.3035091275197125,0.9916321370006804,0.7904799776736032]    |[1.0152371230014354,3.496885433889147,2.725504815599368]    |\n",
      "|[0.5538877734705204,0.7039511562083257,0.1786264357281525]    |[1.852752944200532,2.4824090027576715,0.6158880990296943]   |\n",
      "|[0.7302572874882337,0.20408634026822958,0.3847373028643034]   |[2.4427084406291364,0.7196888078862519,1.326540078577866]   |\n",
      "|[0.8657684270611586,0.8808632046566417,0.6088976579531823]    |[2.8959927968491215,3.1062705560646946,2.099425090870323]   |\n",
      "|[0.9699832791064639,0.6821198129135448,0.5544277644070692]    |[3.2445911649743904,2.405423088807103,1.911617731597251]    |\n",
      "|[0.4446892505222664,0.634706497349268,0.8399247848628077]     |[1.4874842118234757,2.238225066090141,2.895986123041419]    |\n",
      "|[0.4332201064341682,0.6485484291781073,0.3851704231664055]    |[1.4491199591815729,2.28703716886805,1.3280334389442945]    |\n",
      "|[0.9015567375378032,0.24618222669990708,0.3618791258257509]   |[3.0157045882614932,0.8681354814025256,1.2477271125901024]  |\n",
      "|[0.5398873323135149,0.5017633532490794,0.3483828229401209]    |[1.8059215104405506,1.7694151850938613,1.2011930579062269]  |\n",
      "|[0.4858331037079766,0.3112069044552952,0.47542543648990543]   |[1.6251102775659003,1.0974381028100004,1.6392247156280362]  |\n",
      "|[0.7174556601829393,0.7334677772923341,0.6103067002161385]    |[2.3998870356144892,2.5864962327646426,2.1042833435541826]  |\n",
      "|[0.20756808161368057,0.500254054016686,0.5599308866691456]    |[0.6943146116444671,1.7640928016169604,1.9305920088082476]  |\n",
      "|[0.35412171071073806,0.6683139044892138,0.5636162429905227]   |[1.1845360622670769,2.3567380187400184,1.9432987903647823]  |\n",
      "|[0.7261700100268738,0.44803127263029974,0.7093157643630988]   |[2.4290365097561177,1.5799347083752666,2.4456578106402427]  |\n",
      "|[0.31237638307061266,0.0465519418715209,0.9725605650154355]   |[1.0448980662751421,0.1641604798551058,3.3533025228706284]  |\n",
      "|[0.3884065713813064,0.5053582762929301,0.21343279395526016]   |[1.299218818578688,1.7820922994782504,0.7358973334705813]   |\n",
      "|[0.8757763794377906,0.29767676482282446,0.9863357899703395]   |[2.929469367590235,1.049725502104684,3.4007982760977367]    |\n",
      "|[0.002480515968114161,0.14165546310252997,0.32215715374406106]|[0.008297318487938325,0.49953294883354377,1.110769277791595]|\n",
      "+--------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "def generate_random_uniform_df(nrows=10, ncols=10,numPartitions=10):\n",
    "    return RandomRDDs.uniformVectorRDD(spark.sparkContext, nrows,ncols,numPartitions).map(lambda a : a.tolist()).toDF()\n",
    "\n",
    "df=generate_random_uniform_df(nrows=1000,ncols=3,numPartitions=100)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=df.columns, outputCol=\"rawFeatures\")\n",
    "df=vectorAssembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df)\n",
    "scaledData.select(\"rawFeatures\",\"scaledFeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdec44c",
   "metadata": {},
   "source": [
    "###  5)Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d3ae84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|sentence                           |words                                     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f8979f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|sentence                           |words                                     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Regx\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf2bbb",
   "metadata": {},
   "source": [
    "### 6)StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a3f04",
   "metadata": {},
   "source": [
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81d314",
   "metadata": {},
   "source": [
    " id | raw                         | filtered\n",
    "----|-----------------------------|--------------------\n",
    " 0  | [I, saw, the, red, baloon]  |  [saw, red, baloon]\n",
    " 1  | [Mary, had, a, little, lamb]|[Mary, little, lamb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af9672db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------------------------+\n",
      "|sentence                           |filtered                             |\n",
      "+-----------------------------------+-------------------------------------+\n",
      "|Hi I heard about Spark             |[hi, heard, spark]                   |\n",
      "|I wish Java could use case classes |[wish, java, use, case, classes]     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]|\n",
      "+-----------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover_df =remover.transform(tokenized).select(\"sentence\",\"filtered\")\n",
    "remover_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c5031",
   "metadata": {},
   "source": [
    "### 7) n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2aa2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------------------------------------------------------------+\n",
      "|words                                     |ngrams                                                            |\n",
      "+------------------------------------------+------------------------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I, wish, Java, could, use, case, classes]|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic, regression, models, are, neat] |[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"words\",\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c85fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
