{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27510c2",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34188c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/05 09:11:55 WARN Utils: Your hostname, a-Lenovo-Legion-Y530-15ICH resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlp7s0)\n",
      "22/04/05 09:11:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/04/05 09:11:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/05 09:11:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/05 09:11:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/04/05 09:11:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "spark = SparkSession.builder.appName(\"asheesh\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53a764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 7) / 7]\r",
      "[Stage 1:========>                                                  (1 + 6) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----+\n",
      "|genders|ads|age_group|event|\n",
      "+-------+---+---------+-----+\n",
      "|   MALE| a7|     1-15| IMPR|\n",
      "| FEMALE| a1|     1-15|CLICK|\n",
      "|   MALE| a4|    31-45| IMPR|\n",
      "|   MALE| a4|    46-60| IMPR|\n",
      "| FEMALE| a5|     1-15|CLICK|\n",
      "|   MALE| a9|    46-60| IMPR|\n",
      "| FEMALE| a8|    46-60| IMPR|\n",
      "| FEMALE| a7|    46-60|CLICK|\n",
      "|   MALE| a5|    31-45|CLICK|\n",
      "|   MALE| a9|    31-45|CLICK|\n",
      "|   MALE| a8|     1-15| IMPR|\n",
      "| FEMALE| a9|    46-60| IMPR|\n",
      "| FEMALE| a3|    46-60|CLICK|\n",
      "| FEMALE| a4|    16-30| IMPR|\n",
      "| FEMALE|a10|    16-30| IMPR|\n",
      "| FEMALE| a2|    16-30|CLICK|\n",
      "| FEMALE| a5|    16-30|CLICK|\n",
      "| FEMALE| a1|    16-30| IMPR|\n",
      "|   MALE| a1|    46-60| IMPR|\n",
      "|   MALE| a7|     1-15|CLICK|\n",
      "+-------+---+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:==========================================>                (5 + 2) / 7]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Ads Data\n",
    "df = spark.read.csv(\"ads.csv\", header=\"true\", inferSchema=\"true\")\n",
    "cols=df.columns\n",
    "cols.remove(\"_c0\")\n",
    "new_cols=cols.copy()\n",
    "new_cols.remove(\"event\")\n",
    "df=df.select(cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121866e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "splits = df.randomSplit([0.7, 0.3])\n",
    "train_df,test_df = splits[0],splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c90cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the data\n",
    "genders_indexer = StringIndexer(inputCol=\"genders\", outputCol=\"gendersIndex\")\n",
    "ads_indexer = StringIndexer(inputCol=\"ads\", outputCol=\"adsIndex\")\n",
    "age_group_indexer = StringIndexer(inputCol=\"age_group\", outputCol=\"age_groupIndex\")\n",
    "event_indexer = StringIndexer(inputCol=\"event\", outputCol=\"eventIndex\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"gendersIndex\",\"adsIndex\",\"age_groupIndex\"], outputCol=\"rawFeatures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaebf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol = 'rawFeatures', labelCol = 'eventIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f6a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PipeLine\n",
    "pipeline = Pipeline(stages=[genders_indexer,ads_indexer,age_group_indexer,event_indexer,vectorAssembler,rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "385e9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In-Momery Data Transformation and Random Forest Classifier Model Training\n",
    "pipelineModel = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000e220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 93.56% for 8 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 83.17% for 9 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 74.85% for 10 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 68.05% for 11 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 62.37% for 12 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 68.05% for 11 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 74.85% for 10 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 83.17% for 9 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 93.56% for 8 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 93.56% for 8 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 83.17% for 9 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 74.85% for 10 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 68.05% for 11 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 62.37% for 12 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 68.05% for 11 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 74.85% for 10 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 83.17% for 9 writers\n",
      "22/04/05 09:12:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,004,614,439 bytes) of heap memory\n",
      "Scaling row group sizes to 93.56% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Save pipeline\n",
    "pipelineModel.write().overwrite().save(\"random_forest_classifier\")\n",
    "# dir(pipelineModel.write().overwrite())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7c239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxrwxr-x 2 a a 4096 Apr  5 09:12 metadata\r\n",
      "drwxrwxr-x 8 a a 4096 Apr  5 09:12 stages\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lrt random_forest_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27e50ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+----------+\n",
      "|genders|ads|age_group|prediction|\n",
      "+-------+---+---------+----------+\n",
      "|FEMALE |a3 |1-15     |0.0       |\n",
      "|MALE   |a2 |1-15     |1.0       |\n",
      "|FEMALE |a2 |46-60    |1.0       |\n",
      "|FEMALE |a3 |16-30    |1.0       |\n",
      "|MALE   |a5 |16-30    |1.0       |\n",
      "|FEMALE |a10|16-30    |1.0       |\n",
      "|FEMALE |a5 |31-45    |1.0       |\n",
      "|MALE   |a2 |46-60    |1.0       |\n",
      "|MALE   |a1 |46-60    |1.0       |\n",
      "|MALE   |a4 |46-60    |1.0       |\n",
      "|MALE   |a6 |1-15     |0.0       |\n",
      "|MALE   |a1 |16-30    |1.0       |\n",
      "|MALE   |a6 |16-30    |0.0       |\n",
      "|MALE   |a6 |31-45    |0.0       |\n",
      "|FEMALE |a8 |16-30    |1.0       |\n",
      "|MALE   |a9 |16-30    |0.0       |\n",
      "|MALE   |a8 |31-45    |1.0       |\n",
      "|MALE   |a5 |31-45    |1.0       |\n",
      "|MALE   |a2 |16-30    |1.0       |\n",
      "|FEMALE |a1 |31-45    |0.0       |\n",
      "+-------+---+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "# new_cols.remove(\"event\")\n",
    "\n",
    "test_df=test_df.groupBy(new_cols).count()\n",
    "predictions = pipelineModel.transform(test_df)\n",
    "predictions.select(new_cols+[\"prediction\"]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f48f6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "# converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_event\")\n",
    "# predictions_df=converter.transform(predictions)\n",
    "# # predictions_df.select(new_cols+[\"predicted_event\"]).show(truncate=False)\n",
    "# predictions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426591e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
